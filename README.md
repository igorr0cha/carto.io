# Projeto de AutomaÃ§Ã£o e Enriquecimento de CartÃ³rios (CNS Scraper)

Este projeto contÃ©m um robÃ´ de automaÃ§Ã£o (RPA/Scraping) desenvolvido em Python para consultar e enriquecer uma base de dados de cartÃ³rios brasileiros, utilizando o cÃ³digo CNS (Cadastro Nacional de Serventias) como chave de busca.

O robÃ´ foi construÃ­do com foco em **ResiliÃªncia**, **IdempotÃªncia** e **Manutenibilidade**, utilizando um banco de dados SQL Server como gerenciador de estado para criar uma fila de processamento robusta e Ã  prova de falhas.

## ğŸ›ï¸ VisÃ£o Geral da Arquitetura (LÃ³gica de Fila de Trabalho)

Em vez de simplesmente ler um CSV e processÃ¡-lo em memÃ³ria (o que Ã© arriscado), este projeto utiliza uma arquitetura de **Fila de Trabalho (Job Queue)** diretamente no banco de dados.

O fluxo Ã© dividido em quatro etapas distintas:

1.  **Setup do Banco (`src/create_table.py`):** Um script de execuÃ§Ã£o Ãºnica que cria a tabela de destino (`dbo.CartoriosConsulta`) no SQL Server. Esta tabela armazena nÃ£o apenas os dados finais, mas tambÃ©m o **status** do processamento.

2.  **PopulaÃ§Ã£o da Fila (`src/populate_cns.py`):** Este script lÃª o arquivo CSV de entrada (`data_input/*.csv`), extrai a lista de CNS e os insere na tabela `dbo.CartoriosConsulta` com um status inicial (ex: "PENDENTE"). Isso transforma o banco de dados em nossa lista de tarefas.

3.  **Processamento do Scraper (`src/main_scraper.py`):** Este Ã© o robÃ´ principal. Ele opera em um loop contÃ­nuo:
    * Ele consulta o banco por todos os CNS com status "PENDENTE".
    * Para cada um, ele tenta executar o scraping no site-alvo.
    * **Se falhar:** Ele atualiza o status no banco para "ERRO" (junto com a mensagem de erro) e continua para o prÃ³ximo.
    * **Se for bem-sucedido:** Ele preenche todos os dados coletados (Nome, EndereÃ§o, etc.) e atualiza o status para "CONCLUIDO".

4.  **ExportaÃ§Ã£o (`src/export_to_excel.py`):** Um script final que consulta o banco por todos os registros "CONCLUIDO" e gera o arquivo Excel (`.xlsx`) solicitado na pasta `data_output/`.

### Vantagens desta Arquitetura (SRE)

* **ResiliÃªncia Total:** Se o robÃ´ (`main_scraper.py`) parar por qualquer motivo (queda de rede, bloqueio de IP, `Ctrl+C`), basta executÃ¡-lo novamente. Ele simplesmente buscarÃ¡ os prÃ³ximos itens "PENDENTE" no banco, retomando o trabalho exatamente de onde parou.
* **Observabilidade:** Ã‰ fÃ¡cil auditar o processo. Uma consulta SQL (`SELECT Status, COUNT(*) FROM dbo.CartoriosConsulta GROUP BY Status`) mostra exatamente o que foi feito, o que falhou e o que falta.
* **Paralelismo (Escala Futura):** Esta arquitetura permite, no futuro, rodar mÃºltiplos instÃ¢ncias do `main_scraper.py` em paralelo. O banco de dados garantirÃ¡ (com `SELECT...WITH (UPDLOCK, READPAST)`) que diferentes robÃ´s nÃ£o peguem o mesmo item "PENDENTE".

---

## ğŸ› ï¸ Tecnologias Utilizadas

* **Linguagem:** Python 3.10+
* **Banco de Dados:** Microsoft SQL Server
* **ConexÃ£o DB:** `pyodbc`
* **ManipulaÃ§Ã£o de Dados:** `pandas` (para leitura do CSV e exportaÃ§Ã£o do Excel)
* **RPA/Scraping:** [Playwright / Selenium] *(Adicione a ferramenta que vocÃª estÃ¡ usando)*
* **Gerenciamento de Segredos:** `python-dotenv`

---

## ğŸ“‚ Estrutura de Pastas
```
/carto.io/
â”œâ”€â”€ .venv/ # Ambiente virtual Python
â”œâ”€â”€ data_input/
â”‚ â””â”€â”€ base_cartorios_com_cns.xlsx - cns.csv
â”œâ”€â”€ data_output/
â”‚ â””â”€â”€ (vazio, aqui serÃ£o gerados os relatÃ³rios .xlsx)
â”œâ”€â”€ logs/
â”‚ â””â”€â”€ erros_scraping.log # Log de falhas
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ init.py
â”‚ â”œâ”€â”€ settings.py # Carrega variÃ¡veis de ambiente (conexÃ£o DB)
â”‚ â”œâ”€â”€ db.py # FunÃ§Ãµes utilitÃ¡rias de conexÃ£o com o banco
â”‚ â”œâ”€â”€ create_table.py # PASSO 1: Cria a tabela de destino
â”‚ â”œâ”€â”€ populate_cns.py # PASSO 2: LÃª o CSV e popula a fila no DB
â”‚ â”œâ”€â”€ main_scraper.py # PASSO 3: O robÃ´ principal que processa a fila
â”‚ â””â”€â”€ export_to_excel.py # PASSO 4: Gera o Excel final
â”œâ”€â”€ .env # Arquivo de configuraÃ§Ã£o (NÃƒO VERSIONADO)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt # DependÃªncias do projeto
â””â”€â”€ README.md # Esta documentaÃ§Ã£o
```


---

## ğŸš€ Guia de InstalaÃ§Ã£o e ExecuÃ§Ã£o

Siga estes passos para configurar e rodar o projeto.

### 1. PrÃ©-requisitos

* Python 3.10 ou superior.
* Acesso a um banco de dados SQL Server (local ou remoto).
* Driver Microsoft ODBC para SQL Server instalado na mÃ¡quina.

### 2. InstalaÃ§Ã£o

1.  Clone este repositÃ³rio:
    ```bash
    git clone [URL_DO_SEU_REPOSITORIO]
    cd carto.io
    ```

2.  Crie e ative um ambiente virtual:
    ```bash
    # Windows
    python -m venv .venv
    .\.venv\Scripts\activate
    
    # macOS / Linux
    python3 -m venv .venv
    source .venv/bin/activate
    ```

3.  Instale as dependÃªncias:
    ```bash
    pip install -r requirements.txt
    ```

4.  Configure suas credenciais:
    * Crie um arquivo chamado `.env` na raiz do projeto.
    * Adicione sua string de conexÃ£o do SQL Server dentro dele:
    ```ini
    # Arquivo: .env
    SQL_SERVER_CONN_STRING="DRIVER={ODBC Driver 17 for SQL Server};SERVER=SEU_SERVIDOR;DATABASE=SEU_BANCO_DE_TESTE;UID=SEU_USUARIO;PWD=SUA_SENHA"
    ```
    *(**Importante:** O `.env` estÃ¡ no `.gitignore` e nunca deve ser enviado para o repositÃ³rio).*

### 3. Como Executar (Fluxo de Trabalho)

O projeto Ã© executado em 4 passos sequenciais:

#### Passo 1: Criar a Tabela (ExecuÃ§Ã£o Ãšnica)
Este script cria a tabela `dbo.CartoriosConsulta` no seu banco de dados.
```bash
python src/create_table.py
```

Verifique seu banco de dados (SSMS) para confirmar que a tabela foi criada.

#### Passo 2: Popular a Fila de Trabalho (ExecuÃ§Ã£o Ãšnica)
Este script lÃª o arquivo data_input/base_cartorios_com_cns.xlsx - cns.csv e insere todos os CNS na tabela com o status "PENDENTE".

```bash
python src/populate_cns.py
```
Execute um SELECT COUNT(*) na tabela para confirmar que os ~2.800 registros foram inseridos.

#### Passo 3: Executar o RobÃ´ de Scraping
Este Ã© o script principal. Ele irÃ¡ rodar indefinidamente (ou atÃ© que todos os itens "PENDENTE" sejam processados), consultando o site e atualizando o banco.

```bash
python src/main_scraper.py
```
VocÃª pode parar este script (Ctrl+C) a qualquer momento. Para retomar, basta executÃ¡-lo novamente.

#### Passo 4: Gerar o RelatÃ³rio Final
ApÃ³s o main_scraper.py concluir (ou quando vocÃª desejar um relatÃ³rio parcial), execute este script.

```bash
python src/export_to_excel.py
```

---

# âš ï¸ ObservaÃ§Ãµes Importantes:
Substitua [URL_DO_SEU_REPOSITORIO], SEU_SERVIDOR, SEU_BANCO_DE_TESTE, SEU_USUARIO e SUA_SENHA pelos valores reais do seu ambiente.
Certifique-se de que o arquivo de entrada em data_input/ estÃ¡ no formato correto (CSV com coluna CNS).
O scraper estÃ¡ configurado para respeitar os termos de uso do site-alvo. Adicione delays ou proxies conforme necessÃ¡rio para evitar bloqueios.
Os logs de erro sÃ£o salvos em logs/erros_scraping.log para facilitar a anÃ¡lise de falhas.

---

# ğŸ“„ LicenÃ§a
Este projeto Ã© de uso interno e nÃ£o possui licenÃ§a pÃºblica. Todos os direitos reservados.
```
